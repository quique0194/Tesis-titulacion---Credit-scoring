\chapter{Marco Teórico y Estado del Arte}

\ac{CS} consiste en un conjunto de modelos de decisión que ayudan a los prestamistas a otorgar créditos. Estas técnicas son usadas para decidir quién obtiene crédito, cuánto crédito deben obtener, a qué precio lo tendrán, y qué estrategias operacionales mejorarán la rentabilidad de los clientes. Un \ac{CS} puede ser transformada en la probabilidad del cliente de no pagar el préstamo. Así, desde el año 2000, se ha convertido en un pilar de los modelos desarrollados por instituciones financieras cumplir los requerimientos del Acuerdo de Basilea que dictamina que se deben estimar las probabilidades de default de cada portafolio de préstamos \cite[1]{thomas2017credit}.

Con los avances en el concepto de minería de datos y Big Data, ahora hay calificaciones crediticias siendo desarrolladas usando fuentes alternativas de datos. Estas pueden ser psicométricas, data ``social'' en línea, o data del teléfono móvil. Sin embargo, a parte de la consideración de por qué esta información debería ser predictiva del comportamiento crediticio, hay una mayor incertidumbre respecto al hecho de quiénes realmente poseen este tipo de información \cite[18]{thomas2017credit}.

Algunos desafíos claramente identificados en la creación de calificaciones crediticias están relacionados con la disponibilidad de la data, su exactitud y su confiabilidad \cite[18--19]{thomas2017credit}. Sin embargo no trataremos estos temas porque están fuera del ámbito de esta investigación.

Cuando el \ac{CS} se desarrolló por primera vez en los años 1950s y 1960s, los únicos métodos utilizados fueron de discriminación estadística. Aún hoy, con el advenimiento de muchos nuevos enfoques de clasificación basados en minería de datos, los métodos estadísticos, especialmente la regresión logística son por mucho los métodos más utilizados para construir calificaciones crediticias \cite[25]{thomas2017credit}. Tienen la ventaja de permitir el uso de herramientas como los intervalos de confianza y prueba de hipótesis en el contexto de \ac{CS}. Así, uno es capaz de comentar el poder discrimatorio del modelo construido y la importancia relativa de la diferentes características (variables) que constituyen el modelo y su discriminación. De esta forma, uno es capaz de usar esta información para proponer cambios en las preguntas hechas a los prestatarios.

Actualmente numerosas técnicas de clasificación han sido adoptadas para hacer \ac{CS}. Estas técnicas incluyen métodos estadísticos (e.g. análisis discrimante y regresión logística), modelos estadísticos no paramétricos (e.g. K-nearest neighbour y árboles de decisión) y redes neuronales. Con frecuencia surgen conflictos cuando se comparan las conclusiones de algunos de estos estudios. Por ejemplo, en \cite{desai1996comparison} encontraron que las redes neuronales se desempeñan considerablemente mejor que el análisis discriminante al predecir los malos préstamos, mientras que \cite{yobas2000credit} reportó que el segundo supera al primero. Por esto, el problema de escoger una técnica de clasificación para hacer el \ac{CS} permanece como una problema difícil y desafiante \cite{baesens2003benchmarking}.

Los últimos estudios de benchmarking en esta área son del año 2003 \cite{baesens2003benchmarking} y 2015 \cite{lessmann2015benchmarking}. En el último benchmarking de 2015 se compararon 41 métodos de clasificación a través de 8 datasets. Además se creó una tabla muy interesante comparando los algoritmos de clasificación y evaluación utilizados en los últimos estudios. En dicha tabla se evidencia que los algoritmos más utilizados son las \ac{MLP}, las \ac{SVM} y métodos de ensamble. También se ve que los algoritmos de evaluación más utilizados son el \ac{AUC} y aquellos que involucran un umbral, como error de clasificación, \ac{TPR}, etc. También se incluye información sobre qué estudios utilizaron una validación estadística de la hipótesis.

A pesar de tener mucha investigación, la literatura en \ac{CS} no refleja varios recientes avances en aprendizaje predictivo \cite{lessmann2015benchmarking}. Estos avances conciernen 3 dimensiones:

\begin{itemize}
    \item Nuevos algoritmos de clasificación
    \item Nuevas medidas de desempeño para \textit{evaluar} modelos
    \item Pruebas de hipótesis estadísticas para \textit{comparar} modelos 
\end{itemize}

Un análisis de le literatura de modelamiento de default confirma que los estudios en esta área tienen varias limitaciones \cite{lessmann2015benchmarking}:

\begin{itemize}
    \item Utilizan pocos o pequeños datasets
    \item No comparan diferentes clasificadores de estado del arte entre ellos
    \item Usan un pequeño conjunto de indicadores de exactitud conceptualmente similares
\end{itemize}

A continuación se dará un breve repaso de los algoritmos que se utilizarán en esta investigación, tanto para la clasificación como para la evaluación de los modelos.

\section{Algoritmos de clasificación base}
%En random forest hay que poner la diferencia con decision tree. %https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd

\subsection{Regresión Logística}

Es un modelo estadístico que, en su forma básica, utiliza una función logística para modelar una variable binaria dependiente.

Dado que $y\in \{0, 1\}$ y $x$ sea el vector de entrada, se tiene un vector de parámetros $w$ y un intercepto $w_0$. Para clasificar a $x$ se calcula una combinación lineal de $x$ y $w$, a la que luego se le aplica la función logística para obtener la probabilidad de que $y = 1$, tal como se muestra en la fórmula \ref{eq:lr}.

\begin{equation}
    \label{eq:lr}
\begin{split}
    f(x) &= w_0 + wx \\
    P(y=1|x) &= \frac{1}{1 + \exp(-f(x)) }
\end{split}
\end{equation}

\subsection{Multi Layer Perceptron}

Es la red neuronal más utilizada. Está inspirada por el funcionamiento del cerebro humano. Típicamente, se compone de tres capas, una capa de entrada, una capa de neuronas ocultas y una capa de salida. Cada neurona procesa su entrada y genera una salida que es transmitida a las neuronas en la siguiente capa.

\begin{figure}
    \centering
    \caption{Arquitectura de una MLP típica}
    \label{fig:mlp-eg}
    \includegraphics[width=0.6\linewidth]{graficos/mlp_eg.png}
\end{figure}

Tomando como ejemplo la figura \ref{fig:mlp-eg}, la salida de la neurona oculta $j$ es calculada procesando las entradas multiplicadas por sus respectivos pesos $w_{ij}$ y sumándole un bias $b_j$. Luego a este resultado le aplicamos una función de activación (ver fórmula \ref{eq:mlp-out}) que le permite a la red modelar relaciones no lineales en la data. Esta función de activación suele ser la función logistica, la tangente hiperbólica o la función \ac{ReLU}. Para una comparación entre estas funciones ver figura \ref{fig:mlp-activation}

\begin{equation}
    \label{eq:mlp-out}
    h_j = f(b_j + \sum_{j=1}^{n_h} W_{ij} x_i)
\end{equation}

\begin{figure}
    \centering
    \caption{Funciones de activación}
    \label{fig:mlp-activation}
    \includegraphics[width=0.6\linewidth]{graficos/mlp_activation_functions.png}
\end{figure}

\subsection{Deep Belief Network}

Es un tipo de red neuronal profunda, compuesta por múltiples capas de variables latentes (unidades ocultas), con conexiones entre las capas pero no entre unidades dentro de una misma capa.

El entrenamiento de esta red consiste en una etapa no supervisada seguida de otra supervisada. En la etapa no supervisada la \ac{DBN} aprende a reconstruir probabilísticamente sus entradas. Luego, se mejora la clasificación mediante la siguiente etapa de entrenamiento supervisado.

Las \ac{DBN} pueden ser vistas como la composición de redes simples no supervisadas tales como la \ac{RBM} o los autoencoders, donde la capa oculta de cada sub-red se convierte en la capa visible de la siguiente sub-red. Esta composición permite un proceso de entrenamiento no supervisado rápido capa por capa. Ver Figura \ref{fig:dbn-train}.

\begin{figure}
    \centering
    \caption{Entrenamiento no supervisado de una DBN}
    \label{fig:dbn-train}
    \includegraphics[width=0.75\linewidth]{graficos/dbn_train.png}
\end{figure}

\subsection{Support Vector Machine}

Es una familia de algoritmos de aprendizaje supervisado desarrollados en los laboratorios AT\&T, que pueden hacer clasificación y regresión.

Funcionan construyendo un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta. Cada uno de estos hiperplanos está definido por el vector entre los dos puntos más cercanos de 2 clases diferentes, al que se llama vector de soporte. De esta manera se consigue separar a las clases con un margen máximo. Ver Figura \ref{fig:svm-hiperplanos}.

\begin{figure}
    \centering
    \caption{Hiperplanos creados por una SVM}
    \label{fig:svm-hiperplanos}
    \includegraphics[width=0.6\linewidth]{graficos/svm_hiperplanos.png}
    \par
    \small
    $H_1$ no separa las clases. $H_2$ las separa, pero solo con un margen pequeño. $H_3$ las separa con el margen máximo.
\end{figure}

Lamentablemente en las aplicaciones reales, las clases no suelen ser linealmente separables. Por eso existen las representaciones por medio de funciones de Kernel, que mapean el espacio de entradas $X$ a un espacio de características de mayor dimensionalidad, donde aumenta la capacidad computacional de las máquinas de aprendizaje lineal. Algunos tipos de función de Kernel son el Kernel Polinomial Homogéneo, el Kernel Perceptrón, la función de Base Radial Gaussiana y el Kernel Sigmoide.

Finalmente existe una versión de \ac{SVM} llamada \ac{LS-SVM}, que encuentra la solución resolviendo un conjunto de ecuaciones lineales en lugar de un problema convexo de programación cuadrática.

\subsection{Decision Tree}

Un árbol de decisión es un modelo simple de clasificación, se puede ver un ejemplo en la Figura \ref{fig:dt-eg}. Cada nodo interno corresponde a una de las variables de entrada, y según el valor de esta se puede bajar a alguno de los nodos hijos. Cada hoja representa la predicción dada por los valores de la entrada representados por el camino desde la raíz hasta la hoja.

Los algoritmos para la construcción de árboles suelen trabajar de arriba hacia abajo, escogiendo una variable en cada paso que separa mejor la data. Diferentes algoritmos usan diferentes métricas para definir una ``mejor separación de la data''. Estos típicamente miden la homogeneidad de la variable objetivo en los subsets resultantes. Entre estos algoritmos tenemos la Impureza de Gini, la Ganancia de Información y la Reducción de la Varianza.

Entre las ventajas de los árboles de decisión tenemos que son fáciles de interpretar y requieren poca preparación de los datos. Sin embargo también tiene desventajas, como la tendencia al sobreajuste y la dificultad para aprender relaciones tipo XOR.

\begin{figure}
    \centering
    \caption{Ejemplo de Decision Tree}
    \label{fig:dt-eg}
    \includegraphics[width=0.6\linewidth]{graficos/dt_example.png}
\end{figure}

\section{Métodos de ensamble}

En machine learning, los métodos de ensamble usan múltiples algoritmos de aprendizaje para obtener un mejor poder predictivo del que podría ser conseguido por cualquiera de los algoritmos constituyentes por sí solo.

Evaluar la predicción de un ensamble requiere más computaciones de las que requiere un modelo individual, así que los ensambles pueden ser imaginados como una manera para compensar algoritmos con un pobre aprendizaje realizando mucha computación adicional. Algoritmos rápidos como los árboles de decisión son comúnmente usados en los métodos de ensamble, sin embargo, otros algoritmos más lentos pueden beneficiarse de las técnicas de ensamble también.

\subsection{Random Forest}

Los árboles de decisión son algoritmos muy simples que permiten ver claramente la importancia de cada variable. Sin embargo, son proclives al sobreajuste, y se hace necesario limitar la profundidad de los árboles para mitigar este problema. Además, son muy inestables, ya que con pequeñas variaciones del dataset de entrenamiento se puede generar un árbol completamente distinto. Esto es llamado varianza y se necesitan métodos como el bagging y el boosting para reducirla.

El funcionamiento básico del bagging dentro de Random Forest se puede resumir en pocos pasos (Figura \ref{fig:rf}):

\begin{enumerate}
    \item Asumiendo que tenemos un dataset con 1000 instancias
    \item Creamos varias (e.g. 100) muestras aleatorias con repetición
    \item Entrenamos un árbol de decisión en cada muestra
    \item Dada una nueva instancia para clasificar, calculamos la predicción promedia de todos los árboles entrenados en el paso anterior
\end{enumerate}

\begin{figure}
    \centering
    \caption{Funcionamiento de Random Forest}
    \label{fig:rf}
    \includegraphics[width=\linewidth]{graficos/rf.png}
\end{figure}

Random Forest además incluye una mejora adicional conocida como Random Subspace. En lugar de entrenar los árboles de decisión con la totalidad de las variables, se escoge un subset aleatorio de variables y se entrena el árbol con ellas. Esto resulta en una mayor diversidad que generalmente produce un mejor modelo.

\subsection{XGBoost}

Boosting es un método similar a bagging que agrupa varios modelos débiles en un modelo fuerte. La diferencia con bagging es que no se crean muestras aleatorias con repetición, sino que se incrementa la exactitud del modelo de forma progresiva, haciendo que los módelos débiles se enfoquen más en las instancias mal clasificadas de los clasificadores anteriores. AdaBoost es un ejemplo simple de boosting que se puede resumir en el los siguientes pasos:

\begin{enumerate}
    \item Se escoge una muestra aleatoria del conjunto de entrenamiento
    \item Se entrena un clasificador débil con esta muestra y se clasifica todo el conjunto de entrenamiento
    \item A las instancias mal clasificadas se les da más peso para que tengan más probabilidad de aparecer en la siguiente muestra
    \item Se repite este proceso de manera que las instancias mal clasificadas reciben un mayor enfoque de los clasificadores subsiguientes
\end{enumerate}

Gradient boosting por otro lado, modela el problema de clasificación como una función de pérdida que debe ser minimizada. Cada nuevo clasificador débil es un paso más en la dirección negativa de la gradiente, con lo que se minimiza el error. Está tecnica es susceptible de sufrir sobre ajuste.

Finalmente, XGBoost es una implementación de Gradient Boosting en árboles de decisión creada por Tianqi Chen. La palaba extreme se refiere a la meta de ingeniería de aprovechar los recursos computacionales al límite, lo que provocó que su uso se expandiera rápidamente en la comunidad.

% \section{Recomendaciones generales de escritura}
% Un trabajo de esta naturaleza debe tener en consideración varios aspectos generales:

% \begin{itemize}
% \item Ir de lo genérico a lo específico. Siempre hay qeu considerar que el lector podría ser alguien no muy familiar con el tema 
% y la lectura debe serle atractiva.
% \item No poner frases muy largas. Si las frases son de mas de 2 líneas continuas es probable que la lectura sea dificultosa.
% \item Las figuras, ecuaciones, tablas deben ser citados y explicados {\bf antes} de que aparezcan en el documento.
% \item Encadenar las ideas. Ninguna frase debe estar suelta. Siempre que terminas un párrafo y hay otro a continuación, 
% el primero debe dejar abierta la idea que se explicará a continuación. Todo debe tener secuencia.
% \end{itemize}

\subsection{Método de ensamble para datasets imbalanceados}


Muchos algoritmos de clasificación han demostrado un desempeño no óptimo en problemas imbalanceados \cite{batista2004study, mani2003knn, seiffert2010rusboost}.

El bagging es un método de aprendizaje de ensamble que podría resolver este problema si los clasificadores base fueran certeros y diversos \cite{breiman1996bagging}. Sin embargo, cada clasificador base dentro del bagging aún sufre del mismo problema, puesto que la data se muestrea proporcionalmente del dataset original. Incluso si se balancea el dataset antes de ejecutar los algoritmos de aprendizaje, se altera la distribución original de la data y se hace overfitting.

El método de bagging propuesto \cite{sun2015novel} ataca un problema desbalanceado convirtiéndolo en varios problemas balanceados, lo cual incluye tres componentes: \textit{Balanceo del dataset, Modelamiento y Clasificación}. La figura \ref{fig:bagging-imbalanced} muestra los detalles.

\begin{figure}
    \centering
    \caption{Método de bagging propuesto}
    \label{fig:bagging-imbalanced}
    \includegraphics[width=\linewidth]{graficos/bagging_imbalanced.png}
\end{figure}

En este método, la clase mayoritaria genera varios subsets mediante muestreo sin repetición. Cada subset tiene un número de instancias igual a la clase minoritaria, y luego se combinan con esta. Así, se obtienen varios datasets balanceados (Balanceo del dataset). Luego, cada dataset se usa para crear un clasificador binario con un algoritmo específico (Modelamiento). Finalmente estos clasificadores binarios se combinan en un ensamble para clasificar nueva data (Clasificación).

Esta combinación de los resultados de los clasificadores base se realiza mediante un promedio ponderado por la inversa de la distancia de la instancia a clasificar con el subset utilizado. De esta manera, los subsets más parecidos a la nueva instancia tendrán un mayor peso en la decisión final.


\section{Evaluación de los modelos}

\subsection{Accuracy, Precision y Recall}

Para entender estos conceptos necesitamos conocer primero a la matriz de confusión. Esta es una herramienta que permite la visualización del desempeño de un algoritmo de clasificación. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real. Uno de los beneficios de las matrices de confusión es que facilitan ver si el sistema está confundiendo dos clases. Se puede ver un ejemplo en la figura \ref{fig:confussion}.

\begin{figure}
    \centering
    \caption{Ejemplo de matriz de confusión}
    \label{fig:confussion}
    \includegraphics[width=0.5\linewidth]{graficos/confussion_matrix.png}
\end{figure}

Cuando el problema de clasificación sólo tiene 2 clases, la matriz de confusión se parece más a la figura \ref{fig:confussion2}.

\begin{figure}
    \centering
    \caption{Ejemplo de matriz de confusión con dos clases}
    \label{fig:confussion2}
    \includegraphics[width=0.8\linewidth]{graficos/confussion_matrix_2.png}
\end{figure}

A partir de esta matriz se pueden definir formalmente la accuracy, precision y recall como se ve en la fórmula \ref{eq:accuracy-prec-rec}.

\begin{equation}
    \label{eq:accuracy-prec-rec}
\begin{split}
    \text{Accuracy}  &= \frac{\text{true positive} + \text{true negative}}{\text{true positive} + \text{false positive} + \text{true negative} + \text{false negative}} \\\\
    \text{Precision} &= \frac{\text{true positive}}{\text{true positive} + \text{false positive}} \\\\
    \text{Recall}    &= \frac{\text{true positive}}{\text{true positive} + \text{false negative}} 
\end{split}
\end{equation}

De forma más intuitiva podemos definirlos de la siguiente manera:

\begin{description}
    \item [Accuracy] Porcentaje de clasificaciones correctas
    \item [Precision] De aquellos clasificados como positivos, cuántos son realmente positivos
    \item [Recall] De la población total de positivos, cuántos clasificó como positivos nuestro modelo
\end{description}

Aunque en el ámbito de \ac{CS} lo normal sea que la clase positiva sean los defaults, si invertimos esta situación haciendo que la clase positiva sean las personas que pagan su crédito, entonces la precision y el recall se vuelven métricas muy útiles, con significados muy prácticos:

\begin{description}
    \item [Precision] De las personas que el modelo clasificó como buenos pagadores, cuántos realmente pagan sus préstamos. En otras palabras esta métrica se convierte en $1 - \text{\% de default}$. Y representa muy bien el riesgo crediticio del modelo.
    \item [Recall] Del universo de buenos pagadores, cuántos pueden ser capturados con este modelo. Está métrica tiene una incidencia directa en el costo de adquisión de los clientes.
\end{description}

En otras palabras, el balance entre precisión y recall se convierte en el balance entre el riesgo que se quiere asumir y el costo de adquisición de los clientes y el tamaño del mercado. En cambio el concepto de Accuracy no tiene una incidencia directa en algún aspecto del problema, además de que no es una buena métrica a utilizar en datasets desbalanceados.

Pongamos un ejemplo extremo de desbalance, donde sólo una persona de cada 1000 hace default. Un modelo que simplemente trate de maximizar la accuracy puede aprender erróneamente a clasificar a todos como si pertenecieran a la clase mayoritaria, obteniendo así un accuracy de 0.999.

En estos casos de desbalance tan extremo también la precision y el recall se ven afectados, cosa que se soluciona invirtiendo las clases, i.e. haciendo que 0 sean las personas que pagan y 1 las personas que no pagan adecuadamente. Con esta disposición, y con el mismo ejemplo de tener 1 default en 1000 personas, la precision sería de 0 y el recall también, indicando que nuestro modelo no tiene poder predictivo.

\subsection{Curva ROC y Area bajo la curva (AUC)}

Las métricas vistas en la sección anterior tienen un inconveniente: es necesario encontrar un punto de corte para poder calcularlas. La curva ROC en cambio, evalúa y compara el desempeño de los algoritmos sin necesidad de escoger un punto de corte.

La curva ROC grafica la relación entre la razón de verdaderos positivos y la razón de falsos positivos según se varía el umbral de discriminación. Mientras esta curva se acerque más a la esquina superior izquiera, mejor será la clasificación.

El \ac{AUC} es una medida que nos permite comparar diferentes curvas fácilmente. Funciona calculando el área debajo de la curva ROC, tal como su nombre lo indica. Ver figura \ref{fig:eg-roc}.

\begin{figure}
    \centering
    \caption{Ejemplo de curvas ROC}
    \label{fig:eg-roc}
    \includegraphics[width=0.8\linewidth]{graficos/eg_roc.png}
\end{figure}

Finalmente cabe mencionar que pueden existir dos curvas ROC diferentes que tengan la misma área bajo la curva. Por eso no se puede descartar el uso de las curvas ROC totalmente, ya que dependiendo de a qué se le quiera dar más importancia, se escogerá una curva y otra. Tarea que es imposible solamente observando el \ac{AUC}.

%\section{Repeated-N cross fold validation}


%\section{Pruebas de hipótesis estadísticas}

%\subsection{Pairwise comparison}
%\subsection{Analysis of variance}
%\subsection{Friedman test}
%\subsection{Friedman test with post-hoc test}
%Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning
%Research, 7, 1-30.
%\subsection{Press Q statistic}

\section{Trabajo relacionado}

Actualmente hay mucho trabajo científico buscando mejorar los sistemas de credit scoring mediante diferentes técnicas.

En \cite{sohn2016technology} se propone una regresión logística difusa para explotar mejor atributos de evaluación linguísticos. Los resultados son que efectivamente se mejora el desempeño respecto a una regresión logística normal. En \cite{bahnsen2014example} se propone una regresión logística que usa una matriz de costo dependiendo de cada ejemplo. Los resultados subrayan la importancia de usar los costos financieros reales, ya que se logran mejoras significativas en términos de ahorro de dinero.

Por otro lado en \cite{zhao2015investigation} se aplica una \ac{MLP} que se trata de optimizar mediante (i) la mejora de la distribución de la data usando un método llamado Average Random Choosing, (ii) probando diferentes tamaños de conjuntos de train-test-validation y (iii) encontrar el número óptimo de neuronas ocultas. Los resultados son bastante alentadores y los autores claman que se pueden utilizar incluso fuera del dominio de credit scoring.

En el área de aprendizaje profundo también se ha hecho progreso. Particularmente en \cite{luo2017deep} se hace uso de una \ac{DBN} que se compara con algoritmos más populares en credit scoring como \ac{LR}, \ac{MLP} y \ac{SVM}. Los modelos son aplicados en un dataset de contratos CDS XR 14 (sin reestructuración) y los resultados arrojan que la \ac{DBN} tiene la mejor accuracy y AUC.

En \cite{huang2007credit} se usan tres estrategias para construir modelos híbridos de \ac{SVM}, las pruebas se ejecutan con dos datasets crediticios de la UCI. Comparados con redes neuronales, programación genética y clasificadores de decision tree, los \ac{SVM} tienen una accuracy idéntica con relativamente pocas características. Adicionalmente, al combinar algoritmos genéticos con \ac{SVM}, se puede realizar simultáneamente la selección de variables y la optimización de parámetros del modelo. 

En otro estudio \cite{harris2015credit} se introduce el uso de \ac{CSVM} para calificación crediticia. Este algoritmo resolvería algunas limitaciones \ac{SVM}, particularmente se centra en el costo computacional en datasets grandes, logrando niveles de clasificación similares mientras se mantiene un costo computacional bajo.

En \cite{malekipirbazari2015risk} se propone una clasificación basada en \ac{RF}. Se utiliza el dataset de préstamos peer2peer Lending Club, y los resultados indican que los métodos basado en \ac{RF} superan los score crediticios del FICO así como los propios scores de LC.

Además se presentan algunos métodos nuevos, como el \ac{RF} paralelo de \cite{van2016novel}. Donde al integrar el modelo en el proceso de selección de variables se mejora el average accuracy obteniendo un 76.2\%.

En \cite{xia2017boosted} y \cite{bhatia2017credit} se experimenta con \ac{XGBoost}. En el primer artículo se centra en el ajuste de los parámetros del modelo, mientras que el segundo hace una comparación más amplia con otros modelos como \ac{LDA} y \ac{RF}.

En \cite{zhang2016research} se usa un dataset P2P para construir un modelo basado en decision tree fusionando data de los medios de comunicación sociales. En \cite{zang2014credit} también se usa un dataset P2P, el dataset de Lending Club. Se aplica una \ac{MLP} y se obtienen resultados favorables. Para terminar con P2P, en \cite{tan2018deep} se utiliza una red neuronal profunda para modelar los riesgos en competencia, el del prestamista y el del prestatario, este sería el primer lugar donde se prueba dicho concepto obteniendo un desempeño interesante en las inversiones.

En \cite{nanni2009experimental} se prueban varios clasificadores de ensamble en los datasets de crédito Alemán y Australiano. Dichos clasificadores ensamblados mejoran el desempeño de sus respectivos clasificadores base. En \cite{brown2012experimental} se hace un estudio comparativo de varios modelos en cinco datasets distintos. Se hace un énfasis especial en el imbalanceo de la data, pues se hace un undersampling de la clase minoritaria para pronunciar el imbalance cada vez más y evaluar como se van comportando los modelos en cada caso. Los resultados son que \ac{RF} y clasificadores de Gradient Boosting se desempeñan bien incluso en datasets altamente imbalanceados. Finalmente en \cite{wang2012two} se proponen dos estrategias de ensamble para árboles, que luego se comparan con cinco clasificadores base y cuatro clasificadores de ensamble, obteniendo resultados competitivos.

En conclusión, hasta la fecha se ha venido realizando un trabajo muy diverso, el área de credit scoring es un área muy fértil, pero aún quedan muchas potenciales mejoras por realizarse.

\section{Consideraciones Finales}

Ahora que tenemos una idea clara de cómo funcionan los clasificadores base, los ensambles y los métodos de evaluación; podemos pasar a explicar los datasets utilizados, la metodología y los experimentos realizados.
